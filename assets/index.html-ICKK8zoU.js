import{_ as d,c as o,b as t,o as n}from"./app-OZJ_mqn6.js";const c="/blog/assets/rag_embedding--Icsn3Pg.png",a="/blog/assets/rag_embedding_history-CclI1fHx.png",i={};function l(r,e){return n(),o("div",null,e[0]||(e[0]=[t('<h1 id="介绍" tabindex="-1"><a class="header-anchor" href="#介绍"><span>介绍</span></a></h1><div class="hint-container tip"><p class="hint-container-title">提示</p><p><code>Embedding</code>是AI领域的基石技术，它将抽象符号转化为可计算的数学表示，从而赋能各类机器学习任务。</p></div><h2 id="什么是embedding" tabindex="-1"><a class="header-anchor" href="#什么是embedding"><span>什么是Embedding</span></a></h2><p><code>Embedding</code>是将高维、离散或复杂的数据（如单词、图像、用户行为等）映射到低维连续向量空间的技术。这些向量（即嵌入）能够捕捉原始数据的语义、关系或特征，使机器更容易处理和计算。</p><p><img src="'+c+'" alt="Embedding"></p><h2 id="embedding类型" tabindex="-1"><a class="header-anchor" href="#embedding类型"><span>Embedding类型</span></a></h2><ul><li><code>Word Embedding</code>: 将单词映射为向量，例如<code>Word2Vec</code>、<code>GloVe</code>。</li><li><code>Image Embedding</code>: 用<code>CNN</code>提取图像的向量表示，例如<code>ResNet</code>的特征向量。</li><li><code>Graph Embedding</code>: 将图中的节点/边表示为向量，例如：<code>Node2Vec</code>、<code>GNN</code>。</li><li>用户/商品嵌入(推荐系统)：将用户/商品表示为向量，用于预测偏好，例如：矩阵分解、深度学习。</li></ul><h2 id="embedding维度" tabindex="-1"><a class="header-anchor" href="#embedding维度"><span>Embedding维度</span></a></h2><div class="hint-container tip"><p class="hint-container-title">提示</p><p><code>Embedding</code>维度就是其向量的长度。</p></div><p><code>Embedding</code>维度：即向量的长度，是嵌入技术中的关键参数，直接影响模型的表达能力、计算效率和下游任务效果。</p><p><strong>维度的本质</strong>：是将离散对象(如单词)映射到的连续向量的长度，例如：<code>[0.1, -0.3, ..., 0.8]</code>，理论而言，维度越高，能编码的信息越丰富，但也可能带来冗余和过拟合。</p><p>常见维度设置案例：</p><table><thead><tr><th>分类</th><th>模型</th><th>维度</th></tr></thead><tbody><tr><td><strong>词嵌入</strong></td><td><code>Word2Vec</code>、<code>GloVe</code>、<code>FastText</code></td><td>50/100/200/300/</td></tr><tr><td><strong>推荐系统</strong></td><td>YouTube推荐</td><td>256</td></tr><tr><td><strong>预训练模型</strong></td><td><code>BERT-base</code>、<code>BERT-large</code></td><td>768、1024</td></tr><tr><td><strong>图像嵌入</strong></td><td><code>ResNet-50</code></td><td>2048</td></tr></tbody></table><h2 id="发展历史" tabindex="-1"><a class="header-anchor" href="#发展历史"><span>发展历史</span></a></h2><p><img src="'+a+'" alt="Embedding发展历史"></p><p><strong>初始阶段：分布式表示</strong></p><ul><li>特点：每个词对应一个唯一的<code>ID</code>或<code>One-Hot</code>向量(大多数值为0，某一位为1)。</li><li>局限性：无法表达词与词之间的语义关系(如“狗”和“猫”是独立的)。</li></ul><p><strong>分布式词向量：Word2Vec 时代</strong></p><ul><li>特点：词的意义来自上下文分布，词向量低维稠密，支持词向量计算。</li><li>影响：开启词嵌入大规模训练的时代。</li><li>模型：<code>Word2Vec</code>(2013 Google)、<code>GloVe</code>(2014 Stanford)、<code>FastText</code>(2016 Facebook)。</li></ul><p><strong>上下文相关嵌入：BERT 革命</strong></p><ul><li>特点：词的表示取决于上下文，输出的是句子中每个词的上下文相关表示。</li><li>模型：<code>ELMo</code>(2018 AllenNLP)、<code>BERT</code>(2018 Google)、<code>GPT-2</code>(2019 OpenAI)。</li></ul><p><strong>多模态嵌入</strong></p><ul><li>特点：可以将图像和文本映射到同一向量空间，支持跨语言的嵌入，不同语言语义相近的句子有相近向量表示。</li><li>影响：嵌入模型从单一文本向多模态、任务无关、可解释性发展。</li><li>模型：<code>LoRA</code>(2021)、<code>CLIP</code>(2021 OpenAI)、<code>Flamingo</code>(2022 DeepMind)。</li></ul><p><strong>API和应用时代</strong></p><ul><li>特点：支持全语言、支持超长文本、适配向量数据库。</li><li>模型：<code>BGE</code>(2023 BAAI)、<code>text-embedding-3</code>(2024 OpenAI)。</li></ul>',25)]))}const g=d(i,[["render",l]]),p=JSON.parse('{"path":"/rag/embedding/","title":"介绍","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1748667955000,"contributors":[{"name":"wangtunan","username":"wangtunan","email":"why583440138@gmail.com","commits":1,"url":"https://github.com/wangtunan"}],"changelog":[{"hash":"4cae81a7be7882437e8f42d6762dbb92aa7d76e8","time":1748667955000,"email":"why583440138@gmail.com","author":"wangtunan","message":"blog RAG结构化文档向量存储文章撰写"}]},"filePathRelative":"rag/embedding/README.md"}');export{g as comp,p as data};
